1. The following attributes from the training set do not appear to have a "hump" distribution shape: residual sugar, total sulfur dioxide, density, pH, sulphates, alcohol, and quality. The following attributes appear to have outliers: fixed acidity, volatile acidity, citric acid, residual sugar, free sulfur dioxide, total sulfur dioxide, pH, and sulphates. 

2. The accuracy achieved by ZeroR when run on the training set data is 62.381%. ZeroR is the "dumb" classifier that simply chooses the majority class, meaning that any classifier that we try to build should have a better accuracy than this (62.381%). This can provide us with some sort of baseline of where the accuracy of the new classifier models should fall. 

3. According to the decision tree Weka learned over the training set, the most important attribute is the alcohol percentage. Alcohol is a large determinator of the quality of wine: observing the tree shows that in general, the more alcohol a wine has, the better its quality is.

4. The 10-fold cross-validation is a validation method by dividing the training set into 10 different sets, and training the model on 9 of them, and testing the trained model on the remaining 1 of them. This step is iterated through all 10 of the sets. The percenage of Correctly Classified Instances is 85.98%, which is lower than the 95.873% accuracy of the test using training set. This difference comes from the validation methodlogy, because training the model with the training data set and then testing it again on it, in principle, should give the best output possible (no new instances are added to the test data set from the ones that were used to generate the decision tree). On the other hand, using the 10-fold cross-validation method imposes a more challenging test scenario for the model because the data being tested were not used to generated the decision tree, so it's "new" to the built model. Consequently, the accuracy of the model evaluated using the 10-fold cross-validation is much lower than when it was tested with the training set. Cross-validation is important because it tests the model on data that have not been observed by the learner yet.

5. The command-line for the model I am using is RandomForest -l 100 -K 0 -S 1. The reported accuracy for my model using 10-fold cross-validation is 90.79%. 

6. First I searched up the different algorithms offered by Weka on Google. I wanted to choose the model that works the best and outperforms J48. Since the 10-fold cross-validation method yielded a significantly lower accuracy in comparison to testing with training data, I suspected that overfitting must be occurring during the learning process. I chose RandomForest algorithm because it avoids the overfitting issue of decision tree model by generating multitude of decision trees and then taking the mode of the trees. I tried varying the parameters of the model, changing all the maxDepth, numFeatures, numTrees but none of it significantly changed the accuracy except maxDepth. I set maxDepth to 0 which yielded the best result.

7. In principle, using n-1 folds for cross-validation should yield the least biased predictor when there are n training examples. (leave-one-out cross-validation)

8. I disagree with the statement, because the purpose of model building is not entirely satisfied by substituting it with interpretation of a large dataset. Using machine learning it is still not possible to know why certain things happen. For example, using a decision tree in classifying wines into good or bad wines allows us to (hopefully) correctly classify an unknown instance of wine, but it still doesn't tell us why it is a good or bad wine. This is the fundamental problem that people try to solve by building models and trying to explain them. Just because the interpretation of data allows us to perform certain tasks doesn't mean that building model is entirely obsolete thing to do. In fact, I believe that the two are complementary and not supplementary, since having a large dataset and learning over them can help build a more accurate model which can be then used to help explain why certain things happen. 

9. Classifier A: KStar, Classifier B: J48. wine_acc(A) = 87.46%, wine_acc(B) = 85.98%, car_acc(A) = 87.83%, car_acc(B) = 89.83%. KStar is an instance-based learning algorithm, so it performs better in classifying wines compared to decision trees because the attributes for the wines are continuous dataset whereas the attributes for the cars are discrete. KStar will perform better when trying to classify continuous data because it finds the nearest distance, which is a better indicator than trying to come up with a threshold value like the way decision tree classifies. Therefore, it performs better when trying to classify a continuous dataset like the wine data. Similarly, decision tree outperforms KStar when it comes to the car dataset because the features for the car are discrete, and decision tree is more simple and better approach to classifying discrete dataset by nature.

10. The key difference in the output space for the car task compared to the wine task is that the output space is four dimensional, unlike the wine task in which the output space is only two dimensional.
